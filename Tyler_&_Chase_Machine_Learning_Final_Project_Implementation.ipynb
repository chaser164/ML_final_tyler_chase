{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo3xc-LKtfI8"
      },
      "source": [
        "# PuppyFinder: A Dog Image Classification Project\n",
        "\n",
        "*Tyler Tan & Chase Reynders*\n",
        "\n",
        "## Introduction\n",
        "\n",
        "We are using the [Stanford Dogs Dataset](https://www.kaggle.com/datasets/jessicali9530/stanford-dogs-dataset?resource=download), downloaded from Kaggle.\n",
        "\n",
        "Before running all chunks, **PLEASE FOLLOW THE SETUP SECTION TO LOAD THE DATASET**. Thank you!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veNdcuW4FLF3"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, import the relevant packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "F6FA2r1ECOu2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Image\n",
        "import os\n",
        "import random\n",
        "import statistics\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YPK_vx8NkWE",
        "outputId": "ebe39c2b-3534-4415-db67-242619876ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory \"stanford_dogs\" successfully \"cd\"-ed into. You may proceed!\n"
          ]
        }
      ],
      "source": [
        "# TODO: Ensure this variable is the stanford_dogs directory path.\n",
        "DATASET_PATH = '/Users/chasereynders/Desktop/prac/stanford_dogs'\n",
        "\n",
        "# try to cd to the specified path\n",
        "try:\n",
        "  os.environ['DATASET_PATH'] = DATASET_PATH\n",
        "  os.chdir(os.environ['DATASET_PATH'])\n",
        "  print('Directory \"stanford_dogs\" successfully \"cd\"-ed into. You may proceed!')\n",
        "except:\n",
        "  print('Invalid path :(')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNRc5be3Pujt"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43AgZOwfP4C8"
      },
      "source": [
        "### Dataset Summary Statistics:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9UHVENjCQjj",
        "outputId": "548cfcb5-bd2b-4e59-f1dc-d7af5b4cc558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['n02097658-silky_terrier', 'n02092002-Scottish_deerhound', 'n02099849-Chesapeake_Bay_retriever', 'n02091244-Ibizan_hound', 'n02095314-wire-haired_fox_terrier', 'n02091831-Saluki', 'n02102318-cocker_spaniel', 'n02104365-schipperke', 'n02090622-borzoi', 'n02113023-Pembroke', 'n02105505-komondor', 'n02093256-Staffordshire_bullterrier', 'n02113799-standard_poodle', 'n02109961-Eskimo_dog', 'n02089973-English_foxhound', 'n02099601-golden_retriever', 'n02095889-Sealyham_terrier', 'n02085782-Japanese_spaniel', '.DS_Store', 'n02097047-miniature_schnauzer', 'n02110063-malamute', 'n02105162-malinois', 'n02086079-Pekinese', 'n02097130-giant_schnauzer', 'n02113978-Mexican_hairless', 'n02107142-Doberman', 'n02097209-standard_schnauzer', 'n02115913-dhole', 'n02106662-German_shepherd', 'n02106382-Bouvier_des_Flandres', 'n02110185-Siberian_husky', 'n02094258-Norwich_terrier', 'n02093991-Irish_terrier', 'n02094114-Norfolk_terrier', 'n02109525-Saint_Bernard', 'n02093754-Border_terrier', 'n02105251-briard', 'n02108551-Tibetan_mastiff', 'n02108422-bull_mastiff', 'n02085936-Maltese_dog', 'n02093859-Kerry_blue_terrier', 'n02104029-kuvasz', 'n02107574-Greater_Swiss_Mountain_dog', 'n02095570-Lakeland_terrier', 'n02086646-Blenheim_spaniel', 'n02088238-basset', 'n02098286-West_Highland_white_terrier', 'n02085620-Chihuahua', 'n02106166-Border_collie', 'n02090379-redbone', 'n02090721-Irish_wolfhound', 'n02088632-bluetick', 'n02113712-miniature_poodle', 'n02113186-Cardigan', 'n02108000-EntleBucher', 'n02091467-Norwegian_elkhound', 'n02100236-German_short-haired_pointer', 'n02107683-Bernese_mountain_dog', 'n02086910-papillon', 'n02097474-Tibetan_terrier', 'n02101006-Gordon_setter', 'n02093428-American_Staffordshire_terrier', 'n02100583-vizsla', 'n02105412-kelpie', 'n02092339-Weimaraner', 'n02107312-miniature_pinscher', 'n02108089-boxer', 'n02112137-chow', 'n02105641-Old_English_sheepdog', 'n02110958-pug', 'n02087394-Rhodesian_ridgeback', 'n02097298-Scotch_terrier', 'n02086240-Shih-Tzu', 'n02110627-affenpinscher', 'n02091134-whippet', 'n02102480-Sussex_spaniel', 'n02091635-otterhound', 'n02099267-flat-coated_retriever', 'n02100735-English_setter', 'n02091032-Italian_greyhound', 'n02099712-Labrador_retriever', 'n02106030-collie', 'n02096177-cairn', 'n02106550-Rottweiler', 'n02096294-Australian_terrier', 'n02087046-toy_terrier', 'n02105855-Shetland_sheepdog', 'n02116738-African_hunting_dog', 'n02111277-Newfoundland', 'n02089867-Walker_hound', 'n02098413-Lhasa', 'n02088364-beagle', 'n02111889-Samoyed', 'n02109047-Great_Dane', 'n02096051-Airedale', 'n02088466-bloodhound', 'n02100877-Irish_setter', 'n02112350-keeshond', 'n02096437-Dandie_Dinmont', 'n02110806-basenji', 'n02093647-Bedlington_terrier', 'n02107908-Appenzeller', 'n02101556-clumber', 'n02113624-toy_poodle', 'n02111500-Great_Pyrenees', 'n02102040-English_springer', 'n02088094-Afghan_hound', 'n02101388-Brittany_spaniel', 'n02102177-Welsh_springer_spaniel', 'n02096585-Boston_bull', 'n02115641-dingo', 'n02098105-soft-coated_wheaten_terrier', 'n02099429-curly-coated_retriever', 'n02108915-French_bulldog', 'n02102973-Irish_water_spaniel', 'n02112018-Pomeranian', 'n02112706-Brabancon_griffon', 'n02094433-Yorkshire_terrier', 'n02105056-groenendael', 'n02111129-Leonberg', 'n02089078-black-and-tan_coonhound']\n",
            "1. silky terrier: (183 images)\n",
            "2. Scottish deerhound: (232 images)\n",
            "3. Chesapeake Bay retriever: (167 images)\n",
            "4. Ibizan hound: (188 images)\n",
            "5. wire-haired fox terrier: (157 images)\n",
            "6. Saluki: (200 images)\n",
            "7. cocker spaniel: (159 images)\n",
            "8. schipperke: (154 images)\n",
            "9. borzoi: (151 images)\n",
            "10. Pembroke: (181 images)\n",
            "11. komondor: (154 images)\n",
            "12. Staffordshire bullterrier: (155 images)\n",
            "13. standard poodle: (159 images)\n",
            "14. Eskimo dog: (150 images)\n",
            "15. English foxhound: (157 images)\n",
            "16. golden retriever: (150 images)\n",
            "17. Sealyham terrier: (202 images)\n",
            "18. Japanese spaniel: (185 images)\n",
            "19. miniature schnauzer: (154 images)\n",
            "20. malamute: (178 images)\n",
            "21. malinois: (150 images)\n",
            "22. Pekinese: (149 images)\n",
            "23. giant schnauzer: (157 images)\n",
            "24. Mexican hairless: (155 images)\n",
            "25. Doberman: (150 images)\n",
            "26. standard schnauzer: (155 images)\n",
            "27. dhole: (150 images)\n",
            "28. German shepherd: (152 images)\n",
            "29. Bouvier des Flandres: (150 images)\n",
            "30. Siberian husky: (192 images)\n",
            "31. Norwich terrier: (185 images)\n",
            "32. Irish terrier: (169 images)\n",
            "33. Norfolk terrier: (172 images)\n",
            "34. Saint Bernard: (170 images)\n",
            "35. Border terrier: (172 images)\n",
            "36. briard: (152 images)\n",
            "37. Tibetan mastiff: (152 images)\n",
            "38. bull mastiff: (156 images)\n",
            "39. Maltese dog: (252 images)\n",
            "40. Kerry blue terrier: (179 images)\n",
            "41. kuvasz: (150 images)\n",
            "42. Greater Swiss Mountain dog: (168 images)\n",
            "43. Lakeland terrier: (197 images)\n",
            "44. Blenheim spaniel: (188 images)\n",
            "45. basset: (175 images)\n",
            "46. West Highland white terrier: (169 images)\n",
            "47. Chihuahua: (153 images)\n",
            "48. Border collie: (150 images)\n",
            "49. redbone: (148 images)\n",
            "50. Irish wolfhound: (218 images)\n",
            "51. bluetick: (171 images)\n",
            "52. miniature poodle: (155 images)\n",
            "53. Cardigan: (155 images)\n",
            "54. EntleBucher: (202 images)\n",
            "55. Norwegian elkhound: (196 images)\n",
            "56. German short-haired pointer: (152 images)\n",
            "57. Bernese mountain dog: (218 images)\n",
            "58. papillon: (196 images)\n",
            "59. Tibetan terrier: (206 images)\n",
            "60. Gordon setter: (153 images)\n",
            "61. American Staffordshire terrier: (164 images)\n",
            "62. vizsla: (154 images)\n",
            "63. kelpie: (153 images)\n",
            "64. Weimaraner: (160 images)\n",
            "65. miniature pinscher: (184 images)\n",
            "66. boxer: (151 images)\n",
            "67. chow: (196 images)\n",
            "68. Old English sheepdog: (169 images)\n",
            "69. pug: (200 images)\n",
            "70. Rhodesian ridgeback: (172 images)\n",
            "71. Scotch terrier: (158 images)\n",
            "72. Shih-Tzu: (214 images)\n",
            "73. affenpinscher: (150 images)\n",
            "74. whippet: (187 images)\n",
            "75. Sussex spaniel: (151 images)\n",
            "76. otterhound: (151 images)\n",
            "77. flat-coated retriever: (152 images)\n",
            "78. English setter: (161 images)\n",
            "79. Italian greyhound: (182 images)\n",
            "80. Labrador retriever: (171 images)\n",
            "81. collie: (153 images)\n",
            "82. cairn: (197 images)\n",
            "83. Rottweiler: (152 images)\n",
            "84. Australian terrier: (196 images)\n",
            "85. toy terrier: (172 images)\n",
            "86. Shetland sheepdog: (157 images)\n",
            "87. African hunting dog: (169 images)\n",
            "88. Newfoundland: (195 images)\n",
            "89. Walker hound: (153 images)\n",
            "90. Lhasa: (186 images)\n",
            "91. beagle: (195 images)\n",
            "92. Samoyed: (218 images)\n",
            "93. Great Dane: (156 images)\n",
            "94. Airedale: (202 images)\n",
            "95. bloodhound: (187 images)\n",
            "96. Irish setter: (155 images)\n",
            "97. keeshond: (158 images)\n",
            "98. Dandie Dinmont: (180 images)\n",
            "99. basenji: (209 images)\n",
            "100. Bedlington terrier: (182 images)\n",
            "101. Appenzeller: (151 images)\n",
            "102. clumber: (150 images)\n",
            "103. toy poodle: (151 images)\n",
            "104. Great Pyrenees: (213 images)\n",
            "105. English springer: (159 images)\n",
            "106. Afghan hound: (239 images)\n",
            "107. Brittany spaniel: (152 images)\n",
            "108. Welsh springer spaniel: (150 images)\n",
            "109. Boston bull: (182 images)\n",
            "110. dingo: (156 images)\n",
            "111. soft-coated wheaten terrier: (156 images)\n",
            "112. curly-coated retriever: (151 images)\n",
            "113. French bulldog: (159 images)\n",
            "114. Irish water spaniel: (150 images)\n",
            "115. Pomeranian: (219 images)\n",
            "116. Brabancon griffon: (153 images)\n",
            "117. Yorkshire terrier: (164 images)\n",
            "118. groenendael: (150 images)\n",
            "119. Leonberg: (210 images)\n",
            "120. black-and-tan coonhound: (159 images)\n",
            "\n",
            "120 total breeds\n",
            "20581 total images\n",
            "Average image count for each breed: 171.50833333333333\n",
            "Median image count for each breed: 159.5\n",
            "Image count standard deviation: 23.214019175371373\n"
          ]
        }
      ],
      "source": [
        "image_dir = os.path.join(DATASET_PATH, 'images/Images')\n",
        "files = os.listdir('images/Images')\n",
        "\n",
        "print(files)\n",
        "\n",
        "# Dictionary to store breed names and their corresponding image counts\n",
        "breed_counts = {}\n",
        "\n",
        "# Iterate through each file\n",
        "for file in files:\n",
        "    file_path = os.path.join(image_dir, file)\n",
        "    if os.path.isdir(file_path):\n",
        "        # Count the number of jpg files in the directory\n",
        "        jpg_count = sum(1 for filename in os.listdir(file_path))\n",
        "        # Store the breed name and its count in the dictionary\n",
        "        breed_counts[file] = jpg_count\n",
        "\n",
        "# Print the list of files and their corresponding image counts\n",
        "for i, (breed, count) in enumerate(breed_counts.items(), start=1):\n",
        "    print(f'{i}. {breed.split(\"-\", 1)[-1].replace(\"_\", \" \")}: ({count} images)')\n",
        "print()\n",
        "\n",
        "\n",
        "# Some more summary stats\n",
        "average = statistics.mean(breed_counts.values())\n",
        "median = statistics.median(breed_counts.values())\n",
        "std_dev = statistics.stdev(breed_counts.values())\n",
        "\n",
        "print(f'{len(breed_counts)} total breeds')\n",
        "print(f'{sum(breed_counts.values())} total images')\n",
        "print(\"Average image count for each breed:\", average)\n",
        "print(\"Median image count for each breed:\", median)\n",
        "print(\"Image count standard deviation:\", std_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oXnvS_EmsE"
      },
      "source": [
        "## Pre-Processing the Data & Random Sample\n",
        "\n",
        "This code chunk normalizes each image and converts each one to a Tensor, a matrix-like datastructure.\n",
        "\n",
        "Further, training, validation, and testing splits are established, an the datasets are parsed into data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error saving image for n02105855-Shetland_sheepdog/n02105855_2933\n"
          ]
        }
      ],
      "source": [
        "# whippet 2349 misclassified?\n",
        "# error saving image for n02105855-Shetland_sheepdog/n02105855_2933...I manually moved this image uncropped\n",
        "\n",
        "PROCESSED_DATASET_PATH = '/Users/chasereynders/Desktop/prac/processed_stanford_dogs'\n",
        "\n",
        "def count_objects_and_crop(dataset_path, xml_dir, xml_filename, output_directory):\n",
        "    xml_file = os.path.join(dataset_path, f\"annotations/Annotation/{xml_dir}/{xml_filename}\")\n",
        "    image_file = os.path.join(dataset_path, f\"images/Images/{xml_dir}/{xml_filename}.jpg\")\n",
        "\n",
        "    # Parse XML file\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Get image size\n",
        "    image_width = int(root.find(\"size/width\").text)\n",
        "    image_height = int(root.find(\"size/height\").text)\n",
        "\n",
        "    # Get objects and crop images\n",
        "    object_count = 0\n",
        "    for obj in root.findall(\"object\"):\n",
        "        object_count += 1\n",
        "\n",
        "        # Get bounding box coordinates\n",
        "        xmin = int(obj.find(\"bndbox/xmin\").text)\n",
        "        ymin = int(obj.find(\"bndbox/ymin\").text)\n",
        "        xmax = int(obj.find(\"bndbox/xmax\").text)\n",
        "        ymax = int(obj.find(\"bndbox/ymax\").text)\n",
        "\n",
        "        # Crop image\n",
        "        cropped_image = Image.open(image_file).crop((xmin, ymin, xmax, ymax))\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        output_subdirectory = os.path.join(output_directory, obj.find('name').text)\n",
        "        if not os.path.exists(output_subdirectory):\n",
        "            os.makedirs(output_subdirectory)\n",
        "\n",
        "        # Save cropped image\n",
        "        cropped_image_filename = os.path.join(output_subdirectory, f\"{xml_filename}_{object_count}.jpg\")\n",
        "        try:\n",
        "            cropped_image.save(cropped_image_filename)\n",
        "        except:\n",
        "            print(f'error saving image for {xml_dir}/{xml_filename}')\n",
        "\n",
        "    return object_count\n",
        "\n",
        "ANNOTATION_PATH = os.path.join(DATASET_PATH, \"annotations/Annotation\")\n",
        "annotation_dirs = os.listdir(ANNOTATION_PATH)\n",
        "for annotation in annotation_dirs:\n",
        "    if annotation == '.DS_Store':\n",
        "        continue\n",
        "    BREED_PATH = os.path.join(ANNOTATION_PATH, annotation)\n",
        "    breed_annotations = os.listdir(BREED_PATH)\n",
        "    for breed_annotation in breed_annotations:\n",
        "        count_objects_and_crop(dataset_path=DATASET_PATH, xml_dir=annotation, xml_filename=breed_annotation, output_directory=PROCESSED_DATASET_PATH)\n",
        "\n",
        "# Ensure there are 120 classes as is expected\n",
        "assert len(os.listdir(PROCESSED_DATASET_PATH)) == 120\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "92RIHKbOEpZ8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x169290740>\n"
          ]
        }
      ],
      "source": [
        "# Define your own transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize the images to a fixed size\n",
        "    transforms.ToTensor(),           # Convert PIL Image to tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "full_dataset = torchvision.datasets.ImageFolder(root=PROCESSED_DATASET_PATH, transform=transform)\n",
        "\n",
        "# Define the sizes of train, validation, and test sets\n",
        "dataset_size = len(full_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = int(0.1 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Define PyTorch data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "print(test_loader)\n",
        "\n",
        "# Visualize a sample image\n",
        "sample_images, sample_labels = next(iter(train_loader))\n",
        "sample_images = sample_images.numpy()\n",
        "sample_labels = sample_labels.numpy()\n",
        "\n",
        "# Denormalize the images\n",
        "sample_images = sample_images * 0.5 + 0.5\n",
        "\n",
        "# Define classes\n",
        "classes = full_dataset.classes\n",
        "\n",
        "# # Plot the images\n",
        "# fig, axes = plt.subplots(1, len(sample_images), figsize=(12, 4))\n",
        "# for idx, (image, label) in enumerate(zip(sample_images, sample_labels)):\n",
        "#     axes[idx].imshow(np.transpose(image, (1, 2, 0)))\n",
        "#     axes[idx].set_title(classes[label])\n",
        "#     axes[idx].axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seg8-BV8IFKT"
      },
      "source": [
        "## Defining the CNN EVERYTHING PAST HERE IS IRRELEVANT FOR NOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "80AsDJ9yIEga"
      },
      "outputs": [],
      "source": [
        "# Based on this: https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-train-model#define-a-convolution-neural-network\n",
        "# and this: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(12)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(12)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(24)\n",
        "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(24)\n",
        "        self.fc1 = nn.Linear(24*58*58, 120) # not sure why this is 106...just printed prev shape to match\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.relu(self.bn1(self.conv1(input)))      \n",
        "        output = F.relu(self.bn2(self.conv2(output)))     \n",
        "        output = self.pool(output)                        \n",
        "        output = F.relu(self.bn4(self.conv4(output)))     \n",
        "        output = F.relu(self.bn5(self.conv5(output)))   \n",
        "        output = output.view(-1, 24*58*58)\n",
        "        output = self.fc1(output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYXU5hAnH8Jt"
      },
      "source": [
        "## Training the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKcxXlbVH-y5",
        "outputId": "57cb3404-4c57-4fa1-c6aa-d58fe62e61a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([100, 3, 128, 128])\n",
            "Output shape: torch.Size([100, 120])\n",
            "Output values: tensor([ 2.0723e-01, -1.3523e-02, -6.9179e-01,  6.0131e-01,  7.0736e-02,\n",
            "        -2.8204e-01, -1.2092e-02,  5.5465e-01, -1.5422e-01, -2.1334e-02,\n",
            "        -6.1688e-01, -6.5450e-01, -1.9237e-01,  2.6872e-01, -8.7564e-02,\n",
            "         4.8747e-01, -2.2140e-01,  7.8064e-01, -2.9377e-01, -8.0403e-01,\n",
            "         4.6455e-02,  2.5214e-01,  3.7372e-01, -6.2058e-01,  1.9267e-01,\n",
            "         3.0592e-01, -7.3406e-02,  1.2681e-01,  3.7211e-01,  1.6903e-01,\n",
            "         1.9455e-01, -1.8075e-01, -1.3363e-01,  3.3185e-01,  4.8277e-02,\n",
            "        -8.5681e-01,  4.6077e-01,  2.9338e-01, -4.1754e-01, -7.4134e-01,\n",
            "        -4.6924e-01,  2.3188e-01, -3.3971e-01,  4.6345e-01,  2.7729e-01,\n",
            "         1.1186e-01, -1.0702e-01,  2.4943e-01, -5.8264e-01,  8.6020e-01,\n",
            "         3.5840e-02, -6.6693e-01,  3.4836e-01, -1.7840e-01,  5.4976e-01,\n",
            "        -3.8613e-02,  2.8090e-01, -4.1441e-01,  3.5906e-01, -1.4291e-02,\n",
            "         8.1734e-02,  5.7621e-01,  3.8503e-01,  2.6742e-01, -1.2555e-01,\n",
            "         3.3069e-01,  5.1709e-02, -2.4316e-01,  6.5761e-01,  7.2032e-01,\n",
            "        -4.7580e-01, -1.6907e-01, -2.6164e-01, -9.0010e-01, -3.9667e-01,\n",
            "         2.7650e-01,  3.1570e-01,  3.2279e-01,  5.7297e-01,  2.1901e-01,\n",
            "        -1.9718e-01,  2.5570e-02,  7.3418e-01,  1.5270e-02, -1.6172e-03,\n",
            "         3.7984e-01, -6.0978e-02, -4.6789e-04,  2.6763e-01,  2.8072e-02,\n",
            "        -1.8104e-01, -3.5099e-02,  3.0150e-01, -5.0927e-01, -7.0882e-01,\n",
            "        -6.9737e-01,  2.9926e-01,  1.8507e-01,  2.3165e-03, -7.8093e-01,\n",
            "         3.4063e-01, -3.4041e-01,  1.2283e-01,  1.2283e-01, -6.5220e-02,\n",
            "        -1.6660e-01,  4.0842e-01,  1.0473e-01, -6.4771e-01,  3.3419e-01,\n",
            "        -5.6072e-01,  3.6835e-01, -1.3063e-01,  1.7804e-01, -2.3652e-01,\n",
            "         2.2076e-01,  5.4289e-01, -2.3512e-01,  1.2476e-01, -1.0891e-01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Loss: 4.842238426208496\n",
            "Input shape: torch.Size([100, 3, 128, 128])\n",
            "Output shape: torch.Size([100, 120])\n",
            "Output values: tensor([ 2.5693e+01, -1.2434e+02, -7.9647e+01, -1.9035e+02, -1.4717e+02,\n",
            "        -1.7512e+01,  1.5323e+01,  3.0335e+01, -1.8990e+02, -1.4005e+00,\n",
            "        -1.0324e+02,  6.0906e+02, -1.4638e+02,  9.5162e+01, -1.6368e+02,\n",
            "         3.7669e+01, -1.5484e+02, -2.5934e+02, -1.5148e+02, -1.0763e+02,\n",
            "        -1.1237e+02, -1.3107e+02, -1.4331e+02, -1.0297e+02, -1.3343e+02,\n",
            "         2.3404e+00,  2.1066e+02, -1.0378e+02,  1.8070e+02, -1.7998e+02,\n",
            "        -3.7722e+01, -1.4141e+02,  2.9980e+01,  3.9035e+01,  1.5025e+01,\n",
            "        -1.5729e+02,  1.5500e+02,  4.7096e+00,  3.5492e+01,  3.4059e+01,\n",
            "        -1.0705e+02,  5.0223e+00,  1.1307e+01, -2.1621e+02,  2.9164e+01,\n",
            "         2.3754e+02, -1.6344e+02,  6.6441e+00,  1.0583e+02,  1.0631e+02,\n",
            "        -1.5219e+02, -7.3522e+01,  4.1485e-01, -1.3686e+01,  2.1799e+01,\n",
            "         6.1221e+00, -1.2994e+02,  2.2166e+02,  2.5638e+01,  4.1205e+02,\n",
            "        -1.2440e+02,  2.1976e+01,  6.4708e+01, -1.5960e+02,  2.2824e+02,\n",
            "        -4.0242e+00, -1.3306e+02,  4.6422e+01, -2.0600e+02,  4.2024e+02,\n",
            "        -1.4208e+02, -1.0143e+02,  2.2827e+02,  2.1679e+02,  1.8085e-01,\n",
            "        -1.2049e+02, -1.6297e+02,  2.5512e+02, -7.7561e+01, -5.7874e+01,\n",
            "        -1.0762e+02, -1.2363e+02, -2.5944e+02, -1.5235e+02,  1.6334e+02,\n",
            "         4.6551e+01,  3.0698e+01, -1.0114e+02,  9.7477e+01,  4.3214e+01,\n",
            "        -1.2806e+02,  3.2912e+01, -1.7868e+02,  2.2758e+02, -8.3372e+01,\n",
            "         3.6526e+02,  3.6992e+02,  3.8366e+01,  1.6616e+02,  2.1325e+02,\n",
            "         1.0331e+02,  2.2886e+02,  7.0103e+01,  6.2217e+01,  7.2109e+01,\n",
            "        -9.9617e+01,  1.8208e+01, -1.4127e+02,  2.2008e+02, -1.3891e+02,\n",
            "        -1.1054e+02,  2.0977e+02, -1.1340e+02, -2.1823e+01,  2.3596e+02,\n",
            "         2.8902e+01, -1.9503e+02,  6.9885e+01, -1.6042e+02,  3.4596e+01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Loss: 515.4796752929688\n",
            "Input shape: torch.Size([100, 3, 128, 128])\n",
            "Output shape: torch.Size([100, 120])\n",
            "Output values: tensor([ 7.8103e+03,  7.1854e+03,  3.0587e+03, -3.7454e+03, -6.5802e+02,\n",
            "         1.3311e+02,  2.6064e+03,  4.4150e+03,  1.5659e+02, -2.2755e+01,\n",
            "         4.3329e+03, -2.0036e+05,  2.0915e+03,  1.7593e+03, -3.2384e+03,\n",
            "         4.1864e+03, -1.1973e+03, -5.1403e+03, -1.2901e+03, -2.1190e+03,\n",
            "        -6.6803e+02,  1.0582e+03,  5.4235e+03,  2.4504e+03, -2.6533e+03,\n",
            "         2.7450e+03,  4.0053e+03, -2.0615e+03,  4.0405e+03, -1.5177e+03,\n",
            "         1.0553e+03, -2.8049e+03,  7.8631e+02,  2.2855e+03,  3.6526e+03,\n",
            "        -3.1235e+03,  4.6630e+03,  1.3818e+03,  2.2899e+03,  3.0683e+03,\n",
            "        -9.4773e+01,  5.3992e+02, -2.7423e+02, -2.2476e+03,  2.5318e+03,\n",
            "         6.8033e+03, -3.2522e+03,  9.6907e+01,  4.6713e+03,  2.0209e+03,\n",
            "        -1.3266e+03,  2.6781e+02,  2.6993e+03,  2.0141e+03,  4.6394e+03,\n",
            "         9.6376e+03, -2.5630e+03,  5.7980e+03,  1.9291e+03,  9.4725e+03,\n",
            "         2.6913e+02,  3.9652e+03,  1.7123e+03,  1.5817e+03,  7.7030e+03,\n",
            "        -4.0415e+02, -2.6219e+03,  6.4032e+02, -4.0569e+03,  7.4542e+03,\n",
            "        -1.0448e+03, -2.0156e+03,  3.9818e+03,  7.5175e+03, -9.5752e+01,\n",
            "        -2.3948e+03, -3.2499e+03,  8.5985e+03, -1.4951e+03,  7.8336e+03,\n",
            "        -2.1279e+03, -2.4384e+03, -5.1730e+03, -3.0149e+03,  3.3889e+03,\n",
            "         3.4522e+03,  1.8984e+02, -2.0159e+03,  2.0034e+03,  1.0159e+03,\n",
            "        -2.5389e+03,  3.0930e+03, -3.5447e+03,  4.3586e+03, -1.6416e+03,\n",
            "         9.0638e+03,  7.2848e+03,  3.5707e+03,  8.7135e+03,  5.1065e+03,\n",
            "         1.5983e+03,  7.8609e+03,  8.7417e+03,  3.4492e+03,  1.1076e+03,\n",
            "        -1.9696e+03,  5.1800e+02, -6.2239e+02,  3.9582e+03, -1.0925e+03,\n",
            "         1.0514e+03,  1.1238e+04, -6.5248e+02,  2.6387e+03,  5.1404e+03,\n",
            "         5.4618e+03, -2.1182e+03,  2.8478e+03, -3.1660e+03,  2.1065e+03],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Loss: 3214.710693359375\n",
            "Input shape: torch.Size([100, 3, 128, 128])\n",
            "Output shape: torch.Size([100, 120])\n",
            "Output values: tensor([ 1.9038e+01,  1.7946e+01,  6.9186e+00, -4.9510e+00, -2.3331e+00,\n",
            "        -3.1505e+00, -9.2000e-01,  5.8575e+00,  6.1709e-01,  1.6838e+00,\n",
            "         6.8247e+00, -3.6672e+02, -5.9550e-02, -2.1946e-01, -4.7553e+00,\n",
            "         8.5620e+00, -2.2573e+00, -6.6073e+00, -2.7404e+00, -2.7328e+00,\n",
            "        -1.1132e+00,  8.0341e+00,  8.9005e+00,  6.4858e+00, -3.7592e+00,\n",
            "         1.9867e+00,  8.0707e+00, -2.8639e+00,  6.3182e+00,  2.1043e+00,\n",
            "        -7.6603e-02, -4.1301e+00, -8.3661e-01,  5.5173e+00,  3.3432e+00,\n",
            "        -4.0657e+00,  1.0906e+01,  1.0979e+00,  7.3221e+00,  8.9765e+00,\n",
            "        -1.9714e+00,  2.0031e+00, -3.0718e+00, -3.9888e+00,  4.3807e+00,\n",
            "         6.9853e+00, -4.4147e+00, -9.8765e-01,  1.0606e+01,  3.2654e-01,\n",
            "        -1.1137e+00,  1.7587e+00, -2.2373e+00,  5.8555e+00,  1.4091e+01,\n",
            "         2.4743e+01, -3.5561e+00,  7.1642e+00,  7.6614e+00,  2.9533e+01,\n",
            "        -2.0674e+00,  5.2215e+00,  2.9969e+00,  6.2910e+00,  1.4194e+01,\n",
            "         2.9112e+00, -3.3505e+00, -1.3463e+00, -5.4338e+00,  1.3221e+01,\n",
            "        -1.7744e+00, -3.1997e+00,  3.0935e+00,  1.9182e+01, -1.8538e-01,\n",
            "        -3.2474e+00, -4.6171e+00,  7.8790e+00, -3.9666e+00,  9.9328e+00,\n",
            "        -2.9035e+00, -3.3501e+00, -7.0809e+00, -3.9924e+00,  7.6550e+00,\n",
            "         2.7388e+00, -8.6869e-01, -2.5895e+00, -1.9991e-01,  8.6878e-01,\n",
            "        -3.4920e+00,  1.5871e+00, -5.0329e+00,  3.6480e+00, -2.2316e+00,\n",
            "         1.3172e+01,  1.7986e+01,  2.7728e+00,  2.0876e+01,  7.3327e+00,\n",
            "         2.8084e+00,  1.7423e+01,  8.1359e+00,  1.6014e+00, -1.2824e-01,\n",
            "        -2.7769e+00, -1.3129e+00, -4.1981e-01,  3.3264e+00, -1.9639e+00,\n",
            "         1.0001e+00,  1.5555e+01,  4.8674e+00,  1.2372e+01,  7.5511e-01,\n",
            "         1.5758e+01, -3.6515e+00,  2.9837e+00, -4.3033e+00,  2.1831e+00],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Loss: 5480.95556640625\n",
            "Input shape: torch.Size([100, 3, 128, 128])\n",
            "Output shape: torch.Size([100, 120])\n",
            "Output values: tensor([ 1.4214e+05,  5.4264e+04,  7.0146e+03, -4.1186e+03,  6.9463e+04,\n",
            "         1.5942e+03,  2.5187e+03,  4.5749e+03,  3.1829e+04,  3.6152e+03,\n",
            "         9.6770e+03, -3.2703e+05,  5.6769e+03,  1.2658e+04, -3.5087e+03,\n",
            "         3.6330e+03, -4.2581e+02,  2.8779e+05,  5.3869e+02, -2.3334e+03,\n",
            "         3.4541e+03,  2.5912e+05,  1.6217e+04,  5.2504e+03, -2.9487e+03,\n",
            "         3.4083e+03,  4.2455e+03, -2.2774e+03,  5.6894e+03,  8.3426e+02,\n",
            "         3.5926e+02,  2.3873e+04,  8.5074e+02,  1.7887e+04,  8.7783e+03,\n",
            "        -3.2618e+03,  6.4727e+03,  4.4850e+03,  1.9815e+03,  2.3164e+03,\n",
            "         2.5347e+03, -9.2461e+02,  8.2744e+02,  1.7816e+04,  7.2821e+04,\n",
            "         6.4489e+03, -9.4098e+02,  7.9029e+04,  7.2620e+03,  1.2545e+03,\n",
            "        -4.1028e+02,  2.6068e+03,  2.1937e+03,  3.1714e+03,  1.1916e+04,\n",
            "        -4.8496e+04,  1.2589e+05,  1.0901e+04,  5.0508e+04, -1.9143e+05,\n",
            "         1.0094e+03,  9.3547e+03,  3.4702e+04,  4.0478e+03,  1.1764e+04,\n",
            "        -3.6646e+02, -7.7224e+02,  1.7458e+03,  9.5402e+03,  4.5066e+03,\n",
            "        -8.6397e+02, -2.2390e+03,  5.7893e+03,  1.1534e+05, -8.0110e+02,\n",
            "        -2.6897e+03, -2.7483e+03,  1.4285e+05, -2.0963e+03,  1.5380e+04,\n",
            "         1.2365e+04, -2.6607e+03, -5.1778e+03,  3.1956e+05,  3.3067e+03,\n",
            "         4.7352e+03,  2.1005e+03, -2.0545e+03,  1.5265e+03,  1.4925e+03,\n",
            "         5.6756e+03,  3.8671e+03, -3.8874e+03,  2.7710e+04, -8.7701e+02,\n",
            "        -4.1900e+04,  1.0561e+04,  5.6138e+03,  1.4702e+04,  1.8646e+05,\n",
            "         2.2884e+03,  1.0900e+04,  6.0163e+04,  1.2999e+04,  2.2109e+03,\n",
            "         2.2613e+03,  1.1109e+05, -1.2447e+03,  2.9838e+03, -3.9621e+02,\n",
            "         1.5894e+03, -1.9639e+06,  9.5250e+01,  7.4928e+03,  5.2927e+03,\n",
            "         1.4442e+04, -2.7864e+03,  5.4435e+03, -2.4931e+03,  3.5568e+04],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "Loss: 615096.6875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1251e4cc0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/chasereynders/Desktop/prac/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/Users/chasereynders/Desktop/prac/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py\", line 1135, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[184], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())             \u001b[38;5;66;03m# Debugging: Print loss\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/prac/.venv/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/prac/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define the model, criterion, optimizer\n",
        "net = Net()\n",
        "net.train()  # Set the model to training mode\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=1.2, momentum=0.5)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "\n",
        "# Training loop with learning rate scheduling\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        print(\"Input shape:\", inputs.shape)  # Debugging: Print input shape\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        print(\"Output shape:\", outputs.shape)  # Debugging: Print output shape\n",
        "        print(\"Output values:\", outputs[0])     # Debugging: Print output values\n",
        "        loss = criterion(outputs, labels)\n",
        "        print(\"Loss:\", loss.item())             # Debugging: Print loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.5f}')\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# transfer learning approach\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "ZwqXEY8wc69u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "torch.save(net.state_dict(), os.path.join(PROCESSED_DATASET_PATH, 'model.pth'))\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# Optionally, you can load the saved model later\n",
        "# loaded_model = Net()\n",
        "# loaded_model.load_state_dict(torch.load('model.pth'))\n",
        "# loaded_model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "predicted class: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[181], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Disable gradient computation for inference\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Iterate over the test data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get the predicted class for each sample\u001b[39;49;00m\n",
            "File \u001b[0;32m~/Desktop/prac/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Desktop/prac/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1318\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1318\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/prac/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1445\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py:1135\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define lists to store predictions and ground truth labels\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "net.eval()\n",
        "net.load_state_dict(torch.load(os.path.join(PROCESSED_DATASET_PATH, 'model.pth')))\n",
        "\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    # Iterate over the test data\n",
        "    for inputs, labels in test_loader:\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        # Get the predicted class for each sample\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        print(f'predicted class: {predicted}')\n",
        "        # Append the predictions and ground truth labels to the lists\n",
        "        all_predictions.extend(predicted.tolist())\n",
        "        all_labels.extend(labels.tolist())\n",
        "\n",
        "# Calculate accuracy\n",
        "correct = sum(1 for pred, label in zip(all_predictions, all_labels) if pred == label)\n",
        "total = len(all_predictions)\n",
        "accuracy = correct / total\n",
        "\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
